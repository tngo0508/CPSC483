{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use read_csv() to load and examine the training and test sets. Unlike most CSV files, the separator is actually ';' rather than ','."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('bank-additional.csv', sep=';')\n",
    "df_test = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "# df_train\n",
    "# df_train.columns\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The training and test DataFrames will need some significant preprocessing before they can be used:\n",
    "- Several of the features are categorical variables and will need to be turned into numbers before they can be used by ML algorithms. The simplest way to accomplish this is to use dummy coding using get_dummies().\n",
    "- Some algorithms (e.g. logistic regression) have problems with collinear features. If you use one-hot encoding, one dummy variable will be a linear combination of the other dummy variables, so be sure to pass drop_first=True.\n",
    "- Per bank-additional-names.txt, the feature duration “should be discarded if the intention is to have a realistic predictive model,” so removed.\n",
    "- The feature y (or y_yes after dummy coding) is the target, so should be removed.\n",
    "- Some algorithms (e.g. KNN and SVM) require non-categorical features to be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data_train = pd.get_dummies(df_train, drop_first=True)\n",
    "prep_data_test = pd.get_dummies(df_test, drop_first=True)\n",
    "# prep_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prep_data_train.drop(columns=['duration', 'y_yes'])\n",
    "test_data = prep_data_test.drop(columns=['duration', 'y_yes'])\n",
    "# train_data.columns\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "dnames = ['train', 'test']\n",
    "# separate categorical and non-categorical features\n",
    "ori_col = df_train.columns\n",
    "cat_col = set([x for x in train_data.columns if x not in ori_col])\n",
    "non_cat_col = [x for x in train_data.columns if x not in cat_col]\n",
    "\n",
    "# standardize non-categorical features\n",
    "scaler = StandardScaler()\n",
    "sdd_train_data = scaler.fit_transform(train_data[non_cat_col])\n",
    "df_sdd_train_data = pd.DataFrame(sdd_train_data, columns=non_cat_col)\n",
    "\n",
    "# combine standardize non-categorical with categorical features\n",
    "p_train_data = pd.concat([df_sdd_train_data, train_data[cat_col]], axis=1)\n",
    "\n",
    "\n",
    "ori_col = df_test.columns\n",
    "cat_col = set([x for x in test_data.columns if x not in ori_col])\n",
    "non_cat_col = [x for x in test_data.columns if x not in cat_col]\n",
    "\n",
    "# standardize non-categorical features\n",
    "scaler = StandardScaler()\n",
    "sdd_test_data = scaler.fit_transform(test_data[non_cat_col])\n",
    "df_sdd_test_data = pd.DataFrame(sdd_test_data, columns=non_cat_col)\n",
    "\n",
    "# combine standardize non-categorical with categorical features\n",
    "p_test_data = pd.concat([df_sdd_test_data, test_data[cat_col]], axis=1)\n",
    "# p_train_data\n",
    "# p_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Fit Naive Bayes, KNN, and SVM classifiers to the training set, then score each classifier on the test set. Which classifier has the highest accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB classifer score =  0.5962901816062931\n",
      "KNN classifer score =  0.8929057006895212\n",
      "SVM classifer score =  0.8929057006895212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# find score for Gaussian Naive Bayes classifer\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(p_train_data, df_train['y'])\n",
    "print('GNB classifer score = ', gnb.score(p_test_data, df_test['y']))\n",
    "\n",
    "# find score for KNN classifer\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(p_train_data, df_train['y'])\n",
    "print('KNN classifer score = ', knn.score(p_test_data, df_test['y']))\n",
    "\n",
    "# find score for SVM classifer\n",
    "svm = KNeighborsClassifier()\n",
    "svm.fit(p_train_data, df_train['y'])\n",
    "print('SVM classifer score = ', svm.score(p_test_data, df_test['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
