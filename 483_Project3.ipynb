{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "483_Project3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDpfmDEM7Ett"
      },
      "source": [
        "* Thomas Ngo\n",
        "* Tevin Vu\n",
        "* Rachana Chittari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMuqc_qqf-Lm"
      },
      "source": [
        "## 1.Load and examine the Boston dataset’s features, target values, and description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq0WSKSifQjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "570cff63-a764-43ac-d174-ff9461ff0485"
      },
      "source": [
        "from sklearn import datasets\n",
        "dataset_boston = datasets.load_boston()\n",
        "\n",
        "import pandas as pd\n",
        "boston_features= pd.DataFrame(data = dataset_boston.data, columns = dataset_boston.feature_names)\n",
        "boston_features.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476iqnpRgLGr"
      },
      "source": [
        "## 2. Save CRIM as the new target value t, and drop the column CRIM from X. Add the target value MEDV to X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebLh-rZzgM7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbf58e9-b4f7-4258-bb1b-73a604b6cdcd"
      },
      "source": [
        "# MEDV = dataset_boston.target\n",
        "# boston_features['MEDV'] = MEDV\n",
        "# t = boston_features['CRIM']\n",
        "# boston_features = boston_features.drop(columns='CRIM')\n",
        "# X = boston_features\n",
        "# print(X)\n",
        "# print(t)\n",
        "\n",
        "import numpy as np\n",
        "target = dataset_boston.target\n",
        "t = dataset_boston.data[:,0].reshape([-1,1])\n",
        "X = dataset_boston.data[:,1:]\n",
        "X = np.hstack((X, target.reshape([-1,1])))\n",
        "X\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 18.  ,   2.31,   0.  , ..., 396.9 ,   4.98,  24.  ],\n",
              "       [  0.  ,   7.07,   0.  , ..., 396.9 ,   9.14,  21.6 ],\n",
              "       [  0.  ,   7.07,   0.  , ..., 392.83,   4.03,  34.7 ],\n",
              "       ...,\n",
              "       [  0.  ,  11.93,   0.  , ..., 396.9 ,   5.64,  23.9 ],\n",
              "       [  0.  ,  11.93,   0.  , ..., 393.45,   6.48,  22.  ],\n",
              "       [  0.  ,  11.93,   0.  , ..., 396.9 ,   7.88,  11.9 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_OcqqUSgOA9"
      },
      "source": [
        "## 3. Use sklearn.model_selection.train_test_split() to split the features and target values into separate training and test sets. Use 80% of the original data as a training set, and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mOtT5CgS6h"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, t, test_size=0.2, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xt6WKvOgTny"
      },
      "source": [
        "## 4.Create and fit() an sklearn.linear_model.LinearRegression to the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0Dg4CRgWa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3580b9c9-cebf-4965-f33c-07c39afcd217"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train, y_train)\n",
        "\n",
        "print(f'w0 = {lm.intercept_}')\n",
        "print(f'w1 = {lm.coef_}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0 = [-2.18165952]\n",
            "w1 = [[ 1.69425121e-02  3.64829917e-02 -1.34507674e+00 -6.16692078e+00\n",
            "   1.91988972e+00 -1.79735462e-02 -4.93305034e-01  5.80044249e-01\n",
            "   8.43692496e-04 -2.10176913e-01 -6.15238872e-04  1.71179608e-01\n",
            "  -1.73462165e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4MgJhM3h0Yl"
      },
      "source": [
        "## 5. Use the predict() method of the model to find the response for each value in the test set, and sklearn.metrics.mean_squared_error(), to find the training and test MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjxRtUNh2cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b374ed9-a50f-46e8-c0c2-d95592e1f719"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "t_pred_test = lm.predict(X_test)\n",
        "# t_pred_test\n",
        "MSE_test = mean_squared_error(y_test, t_pred_test)\n",
        "print(f'MSE_test = {MSE_test}')\n",
        "\n",
        "t_pred_train = lm.predict(X_train)\n",
        "MSE_train = mean_squared_error(y_train, t_pred_train)\n",
        "print(f'MSE_train = {MSE_train}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE_test = 131.0348526498113\n",
            "MSE_train = 19.54723929709358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUlUtMYgkinJ"
      },
      "source": [
        "## 6. By itself, the MSE doesn’t tell us much. Use the score() method of the model to find the $ R^2 $ values for the training and test data.\n",
        "$ R^2 $, the coefficient of determination, measures the proportion of variability in the target t that can be explained using the features in X. A value near 1 indicates that most of the variability in the response has been explained by the regression, while a value near 0 indicates that the regression does not explain much of the variability. See Section 3.1.3 of An Introduction to Statistical Learning for details.\n",
        "Given the $ R^2 $ scores, how well did our model do?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C_mWvIkl4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da207ad-5cf0-4d1d-ad6e-d3f93fe4a3ca"
      },
      "source": [
        "RR_train = lm.score(X_train, y_train)\n",
        "print(f'RR_train = {RR_train}')\n",
        "\n",
        "RR_test = lm.score(X_test, y_test)\n",
        "print(f'RR_test = {RR_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RR_train = 0.4933215526474889\n",
            "RR_test = 0.16472574847474875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mchqbCPqwm3"
      },
      "source": [
        "Base on the $ R^2 $ scores on the training and test set. The RR_train = 0.49 greater than 0 and less than 1.  it did good on the training but it didn't do well on the test set since $ R^2 $ score getting closer to 0. So that our model didn't do well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwmHHyuDut4T"
      },
      "source": [
        "## 7. Let’s see if we can fit the data better with a more flexible model. Scikit-learn can construct polynomial features for us using sklearn.preprocessing.PolynomialFeatures (though note that this includes interaction features as well; you saw in Project 2 that purely polynomial features can easily be constructed using numpy.hstack()).\n",
        "Add degree-2 polynomial features, then fit a new linear model. Compare the training and test MSE and $ R^2 $ scores. Do we seem to be overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ1INPxpuy0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea50827-1b18-4ce9-dae4-4967f61fcace"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "# calculate train MSE\n",
        "XX_train = poly.fit_transform(X_train)\n",
        "poly_reg = LinearRegression(fit_intercept=True)\n",
        "poly_reg.fit(XX_train, y_train)\n",
        "t_pred_poly_train = poly_reg.predict(XX_train)\n",
        "MSE_poly_train = mean_squared_error(y_train, t_pred_poly_train)\n",
        "print(f'MSE_poly_train = {MSE_poly_train}')\n",
        "\n",
        "# calculate test MSE\n",
        "XX_test = poly.fit_transform(X_test)\n",
        "\n",
        "t_pred_poly_test = poly_reg.predict(XX_test)\n",
        "MSE_poly_test = mean_squared_error(y_test, t_pred_poly_test)\n",
        "print(f'MSE_poly_test = {MSE_poly_test}')\n",
        "\n",
        "# calculate train R^2 score\n",
        "RR_poly_train = poly_reg.score(XX_train, y_train)\n",
        "print(f'RR_poly_train = {RR_poly_train}')\n",
        "\n",
        "# calculate test R^2 score\n",
        "RR_poly_test = poly_reg.score(XX_test, y_test)\n",
        "print(f'RR_poly_test = {RR_poly_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE_poly_train = 13.170800236901677\n",
            "MSE_poly_test = 1175.430638408717\n",
            "RR_poly_train = 0.6586034215370931\n",
            "RR_poly_test = -6.49271607410094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tne-1-Id5pyz"
      },
      "source": [
        "## Do we seem to be overfitting?\n",
        "Yes, it seems to be overfitting because the test MSE gets worse while the train MSE is getting much better. Besides, the test $ R^2 $ is less than 0 which indicates our model is completely wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6l2ZY9RkGPF"
      },
      "source": [
        "## 8. Regularization would allow us to construct a model of intermediate complexity by penalizing large values for the coefficients. Scikit-learn provides this as sklearn.linear_model.Ridge. The parameter alpha corresponds to 𝜆 as shown in the textbook. For now, leave it set to the default value of 1.0, and fit the model to the degree-2 polynomial features. Don’t forget to normalize your features.\n",
        "Once again, compare the training and test MSE and $ R^2 $ scores. Is this model an improvement?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc-gWTqF7J2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a49975a-479a-4f2d-bfaa-37e14075667e"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "clf = Ridge(alpha=1.0,fit_intercept=True,normalize=True)\n",
        "\n",
        "# calculate train MSE using ridge regression\n",
        "clf.fit(XX_train, y_train)\n",
        "t_pred_ridge_train = clf.predict(XX_train)\n",
        "MSE_ridge_train = mean_squared_error(y_train, t_pred_ridge_train)\n",
        "print(f'MSE_ridge_train = {MSE_ridge_train}')\n",
        "\n",
        "# calculate test MSE using ridge regression\n",
        "t_pred_ridge_test = clf.predict(XX_test)\n",
        "MSE_ridge_test = mean_squared_error(y_test, t_pred_ridge_test)\n",
        "print(f'MSE_ridge_test = {MSE_ridge_test}')\n",
        "\n",
        "# calculate train R^2\n",
        "RR_poly_ridge_train = clf.score(XX_train, y_train)\n",
        "print(f'RR_poly_ridge_train = {RR_poly_ridge_train}')\n",
        "\n",
        "# calculate test R^2\n",
        "RR_poly_ridge_test = clf.score(XX_test, y_test)\n",
        "print(f'RR_poly_ridge_test = {RR_poly_ridge_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE_ridge_train = 18.434128430551798\n",
            "MSE_ridge_test = 124.93250789338353\n",
            "RR_poly_ridge_train = 0.5221741838052021\n",
            "RR_poly_ridge_test = 0.2036247997263746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQjyDetxuGu"
      },
      "source": [
        "## Is this model an improvement?\n",
        "yes, this model is an improvement because the test MSE is substantially decreased and the test $ R^2 $ score indicates that 20% of data make accurate prediction about the crime rate per town. Even though the train MSE is slightly increased and the train $ R^2 $ score is slightly decreased, we think that this is still considered a good model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t195FGOn2lpH"
      },
      "source": [
        "## 9. We used the default penalty value of 1.0 in the previous experiment, but there’s no reason to believe that this is optimal. Use sklearn.linear_model.RidgeCV to find an optimal value for alpha. How does this compare to experiment (8)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G46RVCht2naf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1319616b-136d-44bf-db9c-a5217d0ad849"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "clf_cv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10, 100], normalize=True).fit(XX_train, y_train)\n",
        "\n",
        "# calculate train MSE using ridge regression\n",
        "clf_cv.fit(XX_train, y_train)\n",
        "t_pred_ridge_train = clf_cv.predict(XX_train)\n",
        "MSE_ridge_train = mean_squared_error(y_train, t_pred_ridge_train)\n",
        "print(f'MSE_ridge_train = {MSE_ridge_train}')\n",
        "\n",
        "# calculate test MSE using ridge regression\n",
        "t_pred_ridge_test = clf_cv.predict(XX_test)\n",
        "MSE_ridge_test = mean_squared_error(y_test, t_pred_ridge_test)\n",
        "print(f'MSE_ridge_test = {MSE_ridge_test}')\n",
        "\n",
        "# calculate train R^2\n",
        "#RR_poly_ridge_train = clf_cv.score(XX_train, y_train)\n",
        "print(f'RR_poly_ridge_train = {RR_poly_ridge_train}')\n",
        "\n",
        "# calculate test R^2\n",
        "RR_poly_ridge_test = clf_cv.score(XX_test, y_test)\n",
        "print(f'RR_poly_ridge_test = {RR_poly_ridge_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE_ridge_train = 16.755556246434654\n",
            "MSE_ridge_test = 111.0036388539664\n",
            "RR_poly_ridge_train = 0.5221741838052021\n",
            "RR_poly_ridge_test = 0.29241358703157516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKGWP7Y6n5_w"
      },
      "source": [
        "## the optimal value for alpha is 1e-1 or 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoPBUKcR6PUo"
      },
      "source": [
        "## How does this compare to experiment (8)?\n",
        "It works even better than the results in experiment (8) since we observe that the train and test MSEs are both improved in this experiment. Besides, the $ R^2 $ scores shows higher value meaning more percentage of data can be used to make accurate prediction about the crime rate per town."
      ]
    }
  ]
}